{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b22f1c",
   "metadata": {},
   "source": [
    "# Whisper ASR Fine-Tuning Pipeline for Punjabi (Indic-ASR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58e11d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 1. Imports\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset, Audio\n",
    "import torch\n",
    "import torchaudio\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44041a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 🧠 2. Environment and GPU Check\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfaa992",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "BuilderConfig 'pa' not found. Available: ['en', 'fa', 'fr', 'es', 'sl', 'kab', 'cy', 'ca', 'de', 'tt', 'ta', 'ru', 'nl', 'it', 'eu', 'tr', 'ar', 'zh-TW', 'br', 'pt', 'eo', 'zh-CN', 'id', 'ia', 'lv', 'ja', 'rw', 'sv-SE', 'cnh', 'et', 'ky', 'ro', 'hsb', 'el', 'cs', 'pl', 'rm-sursilv', 'rm-vallader', 'mn', 'zh-HK', 'ab', 'cv', 'uk', 'mt', 'as', 'ka', 'fy-NL', 'dv', 'pa-IN', 'vi', 'or', 'ga-IE', 'fi', 'hu', 'th', 'lt', 'lg', 'hi', 'bas', 'sk', 'kmr', 'bg', 'kk', 'ba', 'gl', 'ug', 'hy-AM', 'be', 'ur', 'gn', 'sr', 'uz', 'mr', 'da', 'myv', 'nn-NO', 'ha', 'ckb', 'ml', 'mdf', 'sw', 'sat', 'tig', 'ig', 'nan-tw', 'mhr', 'bn', 'tok', 'yue', 'sah', 'mk', 'sc', 'skr', 'ti', 'mrj', 'tw', 'vot', 'az', 'ast', 'ne-NP']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, Audio\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load Common Voice Punjabi from local directory\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmozilla-foundation/common_voice_11_0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpa\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./commonvoice_punjabi\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain[:1\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# <-- this fixes it\u001b[39;00m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mcast_column(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m, Audio(sampling_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m))\n\u001b[0;32m     12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mrename_column(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\datasets\\load.py:2062\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2057\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   2058\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   2059\u001b[0m )\n\u001b[0;32m   2061\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 2062\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   2063\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   2064\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   2065\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   2066\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   2067\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2068\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   2069\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   2070\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   2071\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   2072\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2073\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2074\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2075\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2076\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   2077\u001b[0m )\n\u001b[0;32m   2079\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   2080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\datasets\\load.py:1819\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[0;32m   1817\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m get_dataset_builder_class(dataset_module, dataset_name\u001b[38;5;241m=\u001b[39mdataset_name)\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[1;32m-> 1819\u001b[0m builder_instance: DatasetBuilder \u001b[38;5;241m=\u001b[39m builder_cls(\n\u001b[0;32m   1820\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1821\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[0;32m   1822\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[0;32m   1823\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   1824\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   1825\u001b[0m     \u001b[38;5;28mhash\u001b[39m\u001b[38;5;241m=\u001b[39mdataset_module\u001b[38;5;241m.\u001b[39mhash,\n\u001b[0;32m   1826\u001b[0m     info\u001b[38;5;241m=\u001b[39minfo,\n\u001b[0;32m   1827\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   1828\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   1829\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   1830\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbuilder_kwargs,\n\u001b[0;32m   1831\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1832\u001b[0m )\n\u001b[0;32m   1833\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39m_use_legacy_cache_dir_if_possible(dataset_module)\n\u001b[0;32m   1835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\datasets\\builder.py:343\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[1;34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, repo_id, data_files, data_dir, storage_options, writer_batch_size, **config_kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m     config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data_dir\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_kwargs \u001b[38;5;241m=\u001b[39m config_kwargs\n\u001b[1;32m--> 343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_builder_config(\n\u001b[0;32m    344\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[0;32m    345\u001b[0m     custom_features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m    347\u001b[0m )\n\u001b[0;32m    349\u001b[0m \u001b[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# Prefill datasetinfo\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;66;03m# TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\datasets\\builder.py:570\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[1;34m(self, config_name, custom_features, **config_kwargs)\u001b[0m\n\u001b[0;32m    568\u001b[0m     builder_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mget(config_name)\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS:\n\u001b[1;32m--> 570\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    571\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilderConfig \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found. Available: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    572\u001b[0m         )\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# if not using an existing config, then create a new config on the fly\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m builder_config:\n",
      "\u001b[1;31mValueError\u001b[0m: BuilderConfig 'pa' not found. Available: ['en', 'fa', 'fr', 'es', 'sl', 'kab', 'cy', 'ca', 'de', 'tt', 'ta', 'ru', 'nl', 'it', 'eu', 'tr', 'ar', 'zh-TW', 'br', 'pt', 'eo', 'zh-CN', 'id', 'ia', 'lv', 'ja', 'rw', 'sv-SE', 'cnh', 'et', 'ky', 'ro', 'hsb', 'el', 'cs', 'pl', 'rm-sursilv', 'rm-vallader', 'mn', 'zh-HK', 'ab', 'cv', 'uk', 'mt', 'as', 'ka', 'fy-NL', 'dv', 'pa-IN', 'vi', 'or', 'ga-IE', 'fi', 'hu', 'th', 'lt', 'lg', 'hi', 'bas', 'sk', 'kmr', 'bg', 'kk', 'ba', 'gl', 'ug', 'hy-AM', 'be', 'ur', 'gn', 'sr', 'uz', 'mr', 'da', 'myv', 'nn-NO', 'ha', 'ckb', 'ml', 'mdf', 'sw', 'sat', 'tig', 'ig', 'nan-tw', 'mhr', 'bn', 'tok', 'yue', 'sah', 'mk', 'sc', 'skr', 'ti', 'mrj', 'tw', 'vot', 'az', 'ast', 'ne-NP']"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "\n",
    "# Load Common Voice Punjabi from local directory\n",
    "dataset = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_11_0\",\n",
    "    \"pa\",\n",
    "    data_dir=\"./commonvoice_punjabi\",\n",
    "    split=\"train[:1%]\",\n",
    "    trust_remote_code=True  # <-- this fixes it\n",
    ")\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "dataset = dataset.rename_column(\"sentence\", \"text\")\n",
    "print(\"✅ Common Voice Punjabi loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4366a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧩 4. Load Whisper Model and Processor\n",
    "try:\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "    model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"punjabi\", task=\"transcribe\")\n",
    "    model.config.suppress_tokens = []\n",
    "    print(\"✅ Whisper model and processor loaded\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to load model:\", str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7504e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧼 5. Preprocess Dataset\n",
    "def prepare_dataset(batch):\n",
    "    try:\n",
    "        audio = batch[\"audio\"]\n",
    "        inputs = processor(audio[\"array\"], sampling_rate=16000)\n",
    "        batch[\"input_features\"] = inputs.input_features[0]\n",
    "        with processor.as_target_processor():\n",
    "            batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    except Exception as e:\n",
    "        print(\"Preprocessing error:\", str(e))\n",
    "        batch[\"input_features\"] = []\n",
    "        batch[\"labels\"] = []\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02678d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)\n",
    "    print(\"✅ Preprocessing completed\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Preprocessing failed:\", str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c1a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ 6. Training Arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-punjabi-finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=100,\n",
    "    max_steps=200,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc42d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 7. Data Collator\n",
    "def data_collator(features):\n",
    "    input_features = [{\"input_features\": f[\"input_features\"]} for f in features if f[\"input_features\"]]\n",
    "    label_features = [f[\"labels\"] for f in features if f[\"labels\"]]\n",
    "    batch = processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "    labels_batch = processor.tokenizer.pad({\"input_ids\": label_features}, return_tensors=\"pt\")\n",
    "    labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52188d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📏 8. Metric Calculation (WER)\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691d9798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    wer_score = metric.compute(predictions=pred_str, references=label_str)\n",
    "    print(f\"WER: {wer_score:.4f}\")\n",
    "    return {\"wer\": wer_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a04de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏋️‍♂️ 9. Trainer Initialization\n",
    "try:\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=dataset,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    print(\"✅ Trainer initialized\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to initialize trainer:\", str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6af798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 10. Fine-Tuning the Model\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"✅ Training completed\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Training failed:\", str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 11. Evaluate Final WER on a Few Samples\n",
    "results = []\n",
    "for example in dataset.select(range(5)):\n",
    "    try:\n",
    "        input_data = processor(example[\"input_features\"], return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(input_data[\"input_features\"])\n",
    "        transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        results.append((transcription, example[\"text\"]))\n",
    "        print(\"Ref:\", example[\"text\"])\n",
    "        print(\"Hyp:\", transcription)\n",
    "        print(\"---\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ Inference failed:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c7365",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for example in dataset.select(range(5)):\n",
    "    try:\n",
    "        input_data = processor(example[\"input_features\"], return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(input_data[\"input_features\"])\n",
    "        transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        results.append((transcription.strip(), example[\"text\"].strip()))\n",
    "        print(\"Ref:\", example[\"text\"])\n",
    "        print(\"Hyp:\", transcription)\n",
    "        print(\"---\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ Inference failed:\", str(e))\n",
    "\n",
    "preds, refs = zip(*results)\n",
    "\n",
    "# Compute WER\n",
    "final_wer = metric.compute(predictions=preds, references=refs)\n",
    "print(\"🔍 Final WER on samples:\", final_wer)\n",
    "\n",
    "# Compute Accuracy\n",
    "correct = sum(p.lower() == r.lower() for p, r in zip(preds, refs))\n",
    "accuracy = correct / len(preds)\n",
    "print(\"✅ Accuracy on samples:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
